\chapter{预备知识}
\section{知识库模型研究}
知识库存储的是客观事实的元数据，构建知识库模型是知识表示学习的重要内容。构建知识库模型的主要任务，是寻找构建模型的方法完成对知识库中实体关系进行表达。在近年来，研究者对知识库模型进行了深入的研究，提出了多种知识库模型的构建方法，并对他们的研究成果进行了验证。
\subsection{知识库与建模}
首先我们先给出知识库的定义：
\begin{equation}
	G = (E,R,S)
\end{equation}
在这个式子中$G$表示知识库，其中
\begin{equation}
	E = \{e_1, e_2, \cdots, e_{\mid E \mid}\}
\end{equation}
$E$表示的由知识库中实体组成的集合，$\mid R \mid$表示在知识库中二元关系组成的集合。一般的，我们会用$(h,r,t)$表示一个表达实体间二元关系的三元组，其中$h \in E$，$r \in R$，$t \in E$，对于知识库中的所有$h$和$t$，我们把它称之为头实体和尾实体，$r$则称之为头实体和尾实体的关系。例如(特朗普，总统，美国)就是一个二元关系三元组的实例，它表达的意思是特朗普是美国的总统，这里的特朗普和美国就是头实体与尾实体，总统表示的是特朗普和美国之间的关系。在式子2.1中的$S$表示所有二元关系的三元组集合，记作$S\subseteq E \times R \times E$。对知识库$G$的建模可以看做是对$(E,R,S)$建立模型的研究。

		\begin{figure}[htb]
  \center
  \includegraphics[width=0.55\textwidth]{img/13.jpg}\\
  \caption{知识库}\label{fig:fulladder}
	\end{figure}
	
	目前构建知识库模型主要方法分为两大类，一类是语义网领域的本体构建方法，本体是一种概念模型的建模工具，构建本体虽然比较复杂，目前构造本体的方法大多基于手动或者是手动加半自动的方法，但是这种方法构建出来的本体可以对知识和语义进行表达，而且本体对于在人可读性与机器可读性中间找到了一个平衡点，在两者的可读性上都要比以往的模型要友好。另一种构建知识库模型的方法是基于机器学习的知识库建模，目前基于机器学习方法的知识库模型有几个具有代表性的模型：距离模型、单层神经网络模型、能量模型、双线性模型、张量神经网络模型、矩阵分解模型和翻译模型[]。
	
	距离模型是由(Bordes)早期提出的使用结构表示(structured embedding, SE)进行知识表示的方法，这种方法的主要思想是[]：
	\begin{itemize}
		\item 所有的实体都可以通过使用一个$d$-维的向量空间进行表达，把实体转化作多维向量空间中向量的操作记作“嵌入空间”。第$i$个实体会被转化成一个向量$E_i$，其中$E_i \in \mathcal{R}^d$。
		\item 在被嵌入的向量空间中，对于任意给定的关系类型，都存在一个特定的相似度衡量值，这个衡量值就是关系的向量空间。在判断头实体和尾实体是否满足该关系时，头实体和尾实体会被投射到关系所在的向量空间，在关系的向量空间中，头实体和尾实体的向量距离越近，就表示关系成立的可能性越大。作者对$k^{th}$个给定的关系上的实体对记作$R_k = (R^{lhs}_k, R^{rhs}_k)$，其中$R_k = (R^{lhs}_k$和$R^{rhs}_k)$都是一个$d\times d$的矩阵。因此，在该关系下$R_k = (R^{lhs}_k$和$R^{rhs}_k)$的相似度计算如下：
		\begin{equation}
			S_k(E_i, E_j) = \mid\mid R^{lhs}_k E_i - R^{lhs}_k E_j \mid\mid
		\end{equation}
	\end{itemize}
	
	距离模型的建模方式利用实体在关系的向量空间上的投影进行相似度计算是合理的，但是在投影的时候，距离模型对关系中的头实体和尾实体采用了两个不同的投影矩阵，这种方法难以保证协同性，因此距离模型在链接预测等任务上没有表现出足够优秀的能力。
	
	单层神经网络模型(single layer model, SLM)是Socher等人提出的新的神经网络模型，为了解决距离模型头尾实体投影矩阵不同带来的协同带来的关系刻画误差问题，单层神经网络模型对二元关系三元组使用了如下评分函数：
	\begin{equation}
		g(e_1,R,e_2) = u^T_Rf(M_{r,1}l_h + M_{r,2}l_t)
	\end{equation}
	在评分函数$g(e_1,R,e_2)$中，$M_{r,1}l_h$和$M_{r,2}l_t$是投影矩阵，$u^T_R$是关系$R$的表示向量，$f = tanh$。单层神经网络模型一定程度上优化了距离模型刻画关系不精准的问题，但是却带来了较大的计算量，降低了计算效率。
	
	DISTMUL是Yang等人提出的一种简化的神经嵌入法，对于一个给定的由关系三元组$(e_1, r, e_2)$(定义$e_1$是在关系$r$下的主体，$e_2$是客体)知识库$KB$，DISTMUL的主要想法是对于正向的三元组，根据评分函数(能量函数)可以获得较高的评分(或者是较低的能量)，而不成立的关系三元组在相同的评分函数则只能得到相反的评分(能量)结果。在DISTMUL模型中，关系矩阵被限制为对角矩阵，这种限制虽然增加了矩阵生成的难度，但是这种对角关系矩阵不仅简化了模型的运算复杂度，也提高了模型的语义表达效果。利用DISTMUL模型还可以完成知识表示学习的子任务关系规则的挖掘。关系规则是知识库的一个重要部分，例如，给定一个事实：一个人是出生在纽约，并且纽约是美国的一个城市，那么这个人的国籍就是美国：
	\begin{equation}
		BornInCity(a,b)\wedge CityOfCountry(b,c)\Longrightarrow Nationality(a,c)
	\end{equation}
	
	类似的逻辑规则可以帮助从知识库中获取新知识，而且它也可以优化知识的存储方式，利用规则的存储减少事实的存储，从而减小知识库的体积。更重要的，这些规则可以满足复杂的推理需求[]。DISTMUL模型挖掘关系规则的主要思想是: 如果二元关系三元组$(e_1, r, e_2)$是一个正向例的且二元关系$r$对应的翻译向量$V$满足$y_a + V - y_b \approx 0$，那么应该会有以下规则属性成立：从$y_a + V_1 \approx y_b$和$y_b + V_2 \approx y_c$得出$y_a + (V_1 + V_2) \approx 0$。DISTMUL在挖掘基数为2和基数为3的规则时取得了不错的效果，但是由于算法复杂度较高，在面对更高基数的规则挖掘时耗时增长严重。
	
\subsection{翻译模型的研究}
	2013年由Mikolov领导的一支谷歌研究团队提供了一种对词的向量表示进行运算的方法，这种方法是将深度学习技术引入自然语言处理领域的一项核心技术，Mikolov还提供了一个开源的Word2vec版本，这项技术使得自然语言处理多了一个新的研究方向。受到词向量特性的启发，Bordes等人提出了TransE模型。
	
	TransE是一个为了学习出实体在低纬嵌入空间的基于能量的模型。在TransE中，关系会被认为是到嵌入空间的一个翻译[]。例如假设$(h,l,t)$是一个正向的二元关系三元组，那么在这个模型下尾部实体$t$在空间的嵌入应该约等于头部实体$h$和关系$l$的空间嵌入向量的和：
	\begin{equation}
		h + l \approx t
	\end{equation}
	
	对于一个给定的由三元组$(h,l,t)$构成的集合$S$，其中$h,t \in E$(实体的集合)以及关系$l \in L$(关系集合)，以下算法的目标是学习关系和实体的嵌入向量。TransE根据能量框架定义三元组的能量等价于相似性测量$d(h+l,t)$，在TransE里d可以是曼哈顿距离或者是欧几里得距离。TransE对训练集数据的训练目标是最小化以下函数：
	\begin{equation}
		\mathcal{L} = \sum\limits_{(h,l,t)\in S} \sum\limits_{(h',l,t')\in S'_{(h,l,t)}} [\gamma + d(h+l,t)-d(h'+l,t')]_{+}
	\end{equation}
	
	在以上函数中，$[x]_+$表示$max(0, x)$。由于训练的时候需要生成反例(不成立的二元关系三元组)，这里的反例可以通过正向三元组提取。具体做法是取一正向实例$(h,l,t)$，从三元组中移除头部(或尾部)实体，使用实体集合中选择一个实体$h'$($h' \in E$)组成损坏三元组$(h',l,t)$，其中损坏三元组满足$(h',l,t) \not \in S_{(h,l,t)}$。所以对所有损坏三元组，有：
	\begin{equation}
		S'_{(h,l,t)} = \{(h', l, t\mid h'\in E)\} \cup \{(h,l,t')\mid t' \in E\}
	\end{equation}
	
	TransE选择了随机梯度下降法(Stochastic Gradient Descent，SGD)作为优化方法，随机梯度下降法在进行训练的时候，并不需要对所有的和求梯度，因此随机梯度下降法也不需要在每次循环的时候更新所有的向量，而只需要对一个批次的向量进行求梯度计算就可以更新$theta$值。
	
		\begin{figure}[htb]
  \center
  \includegraphics[width=0.45\textwidth]{img/9.jpg}\\
  \caption{TransE模型}\label{fig:fulladder}
	\end{figure}
	
	TransE在实验中设置两组实验，分别验证模型在实体预测以及链接预测上的能力，与之对比的还有RESCAL，SE，SME(linear)/SME(bilinear)以及LFM等知识库模型，从实验数据可以看出，TransE取得的效果非常优秀，在三组数据集的两种评价方法下，TransE都取得最好的成绩，值得注意的是在一个体量较大的数据集FB1M的测试中，TransE不仅完成了测试，对比以往的模型还获得了较大的提高。但是，TransE方法也有自身的缺点。假设有两组三元组分别为(上海，位于，中国)记为$(h,l,t)$以及(通州，位于，中国)记为$(h,l,t')$。根据TransE的损失函数，训练出来的模型会有$h+l\approx t$ 以及 $h + l \approx t'$，所以我们有$t \approx t'$，也就是说在TransE的表达模型中，“上海”和“通州”的嵌入向量会近似相等，即使这两个实体具有较大的差异。也就意味着，TransE虽然在处理一对一关系的时候有着不错得性能，但是在面对一对多的关系的时候，TransE方法具有缺陷。
	
	受TransE的启发，Wang等人在2014年时基于TransE提出了一种新的翻译模型，称之为TransH。TransH的提出就是为了解决在TransE中无法很好处理地一对多，多对多的关系。TransE在建立关系实体的向量空间的时候关系和实体都被嵌入到了平面空间，这个因素限制了TransE在处理多对多、多对一以及一对多的关系的表达能力。因此，为了解决这个问题，在TransH中关系被嵌入到了超平面空间。
	
			\begin{figure}[htb]
  \center
  \includegraphics[width=0.45\textwidth]{img/10.jpg}\\
  \caption{TransH模型}\label{fig:fulladder}
	\end{figure}
	
	对于一个关系$r$，TransH用超平面$w_r$和在超平面上的向量$d_r$进行表示。特别地，对于一个二元关系三元组$(h,r,t)$，$h$和$t$将会被首先投影到平面$w_r$上，投影后的向量分别被记作$h_{\bot}$和$t_{\bot}$。在TransH中，如果$(h,r,t)$是一个正向三元组，$h_{\bot}$和$t_{\bot}$会被期望能够被超平面上的向量$d_r$联系起来。因此在TansH中的评分函数为：
	\begin{equation}
		\mid\mid h_{\bot} + d_r - t_{\bot} \mid\mid^2_2
	\end{equation}
	通过限制$\mid\mid w_r \mid\mid_2 = 1$，我们可以得到：
	\begin{equation}
		h_{\bot}=h-w_r^{\top}hw_r,\ \ t_{\bot}=t-w_r^{\top}tw_r
	\end{equation}可以得到评分函数：
	\begin{equation}
		f_r(h,t) = \mid\mid (h-w_r^{\top}hw_r)+d_r-(t-w_r^{\top}tw_r) \mid\mid^2_2
	\end{equation}损失函数为：
	\begin{equation}
		\mathcal{L} = \sum\limits_{(h,l,t)\in S} \sum\limits_{(h',l,t')\in S'_{(h,l,t)}} [\gamma + f_r(h,t)-f_{r'}(h',t')]_{+}
	\end{equation}
	
\section{本体的研究}
	语义网的发展依赖于本体的构建，构建本体的目的是对数据进行结构化处理，使得本体易于传播的同时有着较高的机器可读性。描述逻辑是当前语义网发展中本体的理论基础，本体的表达能力由它的描述语言描述逻辑的表达能力决定。描述逻辑根据表达能力的不同，形成了不同子语言。
\subsection{描述逻辑与因特网本体语言}
	描述逻辑(description logic)是一种用于知识表示的逻辑语言和以其为对象的推理方法，主要用于描述概念分类及其概念之间的关系[]。描述逻辑是一阶逻辑的可决定性片段，但是描述逻辑又具有强的表达能力，描述逻辑在表达能力和推理能力之间取得了平衡。
	
			\begin{figure}[htb]
  \center
  \includegraphics[width=0.85\textwidth]{img/11.png}\\
  \caption{本体}\label{fig:fulladder}
	\end{figure}
	
    描述逻辑语言$\mathcal{L}$包含以下三个集合，分别是一个由个体名组成的集合$N_I$，一个由概念名组成集合$N_C$以及一个由二元关系名组成的集合$N_R$。 
	\begin{definition}[描述逻辑词汇表)]
		描述逻辑词汇表$\mathcal{V}$是一个三元组$(N_C,N_R,N_I)$，其中$N_C$是概念名的集合，$N_R$是关系名的集合，$N_I$是个体名的集合。
	\end{definition}
	
	我们把$\mathcal{L}$ 的\emph{语义演绎}表示为$\mathcal{I}=(\Delta^\mathcal{I},\cdot^\mathcal{I})$，其中$\Delta^\mathcal{I}$表示$\mathcal{I}$的域，是一个非空的个体集合，$\cdot^\mathcal{I}$是一个映射函数，这个映射函数可以完成概念名到$\Delta^\mathcal{I}$子集的映射，二元关系到$\Delta^\mathcal{I}\times\Delta^\mathcal{I}$ 子集的映射以及个体到$\Delta^\mathcal{I}$元素的映射。定义如下[]：
	
	\begin{definition}
		一个演绎$\mathcal{I}$是二元组$(\Delta^\mathcal{I},\cdot^\mathcal{I})$，其中$\Delta^\mathcal{I}$被称为域，$\cdot^\mathcal{I}$是一个从$N_I$到$\Delta^\mathcal{I}$的函数。
		\begin{itemize}
			\item $a^{\mathcal{I}} \in \Delta^\mathcal{I}$ 
			\item $A^{\mathcal{I}} \subseteq \Delta^\mathcal{I}$ 
			\item $R^{\mathcal{I}} \subseteq \Delta^\mathcal{I} \times \Delta^\mathcal{I}$ 
			\item $(C_1 \sqcap C_2)^\mathcal{I} \subseteq C_1^{\mathcal{I}} \cap C_2^{\mathcal{I}}$
		\end{itemize}
	\end{definition}
	
	在描述逻辑的基本语言片段$\mathcal{ALC}$(Attributive Language with Complement)中，概念表达方式如下例[]：
	\begin{example}
		令$N_C = \{Male, Female, Person, Student, Professor\}$，$N_R=\{hasChild\}$，可以构建以下概念：
		\begin{table}[!h]
		\centering
		\caption{$\mathcal{ALC}$概念表达示例}
		\begin{tabular}{cc}
		
		Concept & Intended meaning \\
		\hline
		$Male \sqcap Person$ & $\emph{male person}$ \\
		
		$Student \sqcup Professor$  & $\emph{student or professor}$\\

		$Student \sqcap (\not Male)$ & $\emph{non-male student}$\\

		$\exists hasChild.Person \sqsubseteq Person$ & $\emph{something having a child who is a person is also a person}$\\

		$Female \sqcap \forall hasChild.Female$ & $\emph{Female person whose children are all female}$ \\
		
	\end{tabular}
	\end{table}
	\end{example}
	$\mathcal{ALC}$可以表达概念以及个体断言[]:
	
	\begin{enumerate}
		\item $C(a)$: $a$是$C$的实例。
		\item $R(a,b)$: $a$和$b$存在关系$R$。
		\item $C \sqsubseteq D$: $C$是$D$的子概念。
		\item $C \equiv D$: $C$等价于$D$。
	\end{enumerate}

	式(1)和式(2)被称作断言公理，式(3)和式(4)被称作术语公理。在语义网中，由断言公理组成的集合称为 $\emph{assertion box}(ABox)$，由术语公理组成的集合称为$\emph{terminological box}(TBox)$。
	
		\begin{table}[!h]
	\centering
	\caption{DL示例公理的语义}
	\begin{tabular}{|c|c|}
		\hline
		Syntax & Semantics \\
		\hline
		$C \sqsubseteq D$ & $C ^\mathcal{I} \subseteq D^\mathcal{I}$ \\
		\hline
		$C \equiv D$ & $C ^\mathcal{I} = D^\mathcal{I}$ \\
		\hline
		$C(a)$ & $a ^\mathcal{I} \in C^\mathcal{I}$\\
		\hline
        $r(a,b)$ & $\langle a^\mathcal{I}, b^\mathcal{I}\rangle \in r^\mathcal{I}$\\
		\hline
	\end{tabular}
\end{table}
	
	一个DL本体$\mathcal{O}=(\mathcal{T}, \mathcal{A})$包括两个部分，一个是TBox $\mathcal{T}$，它描述术语知识与应用领域相关的背景知识，处理概念的定义，由有限个公理构成，其中有引入新概念名和角色名称的公理，有断言包含关系的公理以及断言觉得可传递角色或功能性角色公理[面向Web 的个性化语义信息检索技术]。另一个是断言知识的集合ABox $\mathcal{A}$，它描述的是TBox词汇表中的个体断言,包括概念类的成员元素，二元关系的成员元素二元组以及二元关系的等价关系。
	
	在本文中，断言知识部分我们仅考虑二元关系的断言，形如$r(a,b)$，其中$r$是$\Delta^\mathcal{I}\times\Delta^\mathcal{I}$ 中的一个二元关系实例，$a$和$b$是$\Delta^\mathcal{I}$中的一个个体。表2.2展示了TBox与ABox的语法以及语义。
    
	
	语法和语义具有对应关系，如下表
    \begin{table}[!h]
	\centering
	\caption{语法以及语义表}
	\begin{tabular}{|c|c|c|}
		\hline
		Constructor & Syntax & Semantics \\
		\hline
		$\emph{top concept}$ & $\top$ & $\vartriangle ^\mathcal{I}$ \\
		\hline
		$\emph{bottom concept}$  & $\bot$ & $\emptyset$ \\
		\hline
		$\emph{conjunction}$ & C $\sqcap$ D & $C ^\mathcal{I} \cap D^\mathcal{I}$\\
		\hline
		$\emph{existential restriction}$ & $\exists r.C$ &  $\{X \in \vartriangle ^\mathcal{I} | \exists y \in \vartriangle ^\mathcal{I} : \left(x, y \right) \in r ^\mathcal{I}  \wedge r \in C^\mathcal{I}\}$ \\
		\hline
		$\emph{general concept inclusion}$ & $C \sqsubseteq D$ & $C ^\mathcal{I} \subseteq D^\mathcal{I}$ \\
		\hline
		$\emph{role inclusion}$ & $r_1 \circ \cdots \circ r_k \subseteq r$ & $r^\mathcal{I}_1 \circ \cdots \circ r^\mathcal{I}_k \subseteq r^\mathcal{I}$ \\
		\hline
	\end{tabular}

\end{table}


\subsection{本体推理问题的研究}
	本体作为一种知识库模型，除了可以存储客观事实，另一个特点是可以存储规则。规则的允许使得本体具有从知识库中获取新知识的能力，在所有的描述逻辑本体中，推理是其中重要的研究任务，本体的推理任务主要有：一致性检测，实例查询，实例计算，子集计算，分类。定义如下：
	
	\begin{definition}{一致性}
		给定一个本体$\mathcal{O}$，判断是否存在一个演绎$\mathcal{I}$满足$\mathcal{O}$。
	\end{definition}
	
		一致性检测是本体最重要的推理任务之一，不一致的本体会导致查询结果的可信度降低，修复不一致本体是一项重要的任务。
	\begin{definition}{实例查询}
		给定一个本体$\mathcal{O}$以及一个概念断言$C(a)$，判断是否满足$\mathcal{O} \models C(a)$。
	\end{definition}
		
		实例查询是本体在应用中使用最多的场景，在向本体了解个体信息的时候，实例查询可以对给定额查询个体通过搜索知识库与推理相结合的方式给出结果。
	\begin{definition}{实例计算}
		给定一个本体$\mathcal{O}$以及概念$C$，计算所有的个体$x$使得$\mathcal{O} \models C(x)$成立。
	\end{definition}
	
	\begin{definition}{子集验证}
		给定一个本体$\mathcal{O}$以及概念断言$C \sqsubseteq D$，判断本体$\mathcal{O}$是否满足$\mathcal{O} \models C \sqsubseteq D$。
	\end{definition}
	
		子集又称上下位关系，是本体分类任务中的一个子任务，在区分概念上下位关系以及计算本体概念层次结构中具有重要作用。
	\begin{definition}{分类}
		给定一个本体$\mathcal{O}$，计算出所有由原子概念组成的二元组$(A,B)$使得$\mathcal{O} \models A \sqsubseteq B$。
	\end{definition}
	

	\begin{example}
		考虑以下本体$\mathcal{O} = (\mathcal{T}, \mathcal{A})$，其中$\mathcal{T}$为：
		\begin{center}
			\item $Father \sqsubseteq Male$
			\item $Male \sqsubseteq Human$
		\end{center}
		
		$\mathcal{A}$为：
		\begin{center}
			\item $Father(Tom)$
			\item $Male(Tony)$
		\end{center}
		
		显然，在本例中本体$\mathcal{O}$是一致的，又我们可以得出$\mathcal{O} \models Father \sqsubseteq Human$，$\mathcal{O} \models Male(Tom)$，$\mathcal{O} \models Human(Tony)$。
	\end{example}
	
	
\section{推理机与决断集的计算}   						

	本体包含TBox与ABox，因此本体具有从已有的知识库中获取新知识的能力，这种本体中没有，但是可以通过推理出来得到的公理就叫做蕴含。一个本体可能会有一个或多个蕴含，在本体的开发中，能够从蕴含逆推出在推理过程相关的公理具有重大的现实意义，这些公理的集合我们称之为决断集。
    \begin{definition}[决断集)]
	

    令$\mathcal{O}$为一个一致的DL本体，且$\mathcal{O} \vDash \alpha$，其中$\alpha$是蕴含。对于$\mathcal{O}$的一个子集$\mathcal{O}'$，如果对于$\mathcal{O}'$的所有子集$\mathcal{O}''$满足$\mathcal{O}'' \nvDash \alpha$且$\mathcal{O}' \vDash \alpha$，那么$\mathcal{O}'$就是本体$\mathcal{O}$ 中对于$\alpha$的一个决断集。
    \end{definition}
	

	
     \begin{example}
        考虑一个一致的DL本体$\mathcal{O}$，其中TBox包含以下三条公理:
             \begin{center}
               \item [(1)]$Girl\sqsubseteq Female$
               \item [(2)]$Female \sqsubseteq Person$
               \item [(3)]$\exists giveBirth.Person \sqsubseteq Female$
             \end{center}
        ABox包含以下两条公理:
            \begin{center}
               \item [(1)]$Female(Mary)$
               \item [(2)]$giveBirth(Lily, Mary)$
             \end{center}
        蕴含为：
             \begin{center}
               \item $Female(Lily)$
             \end{center}
			 
			 对于以上例子，我们根据$Female(Mary)$以及$Female \sqsubseteq Person$可以得出
			 \begin{equation}
				Person(Mary)
			\end{equation}
			 
			 又根据$\exists giveBirth.Person \sqsubseteq Female$，$giveBirth(Lily, Mary)$以及个体断言$Person(Mary)$，我们可以得到：
			 \begin{equation}
				Female(Lily)
			 \end{equation}
			 
			 令：
			 	$$ \mathcal{S}=\left\{
				\begin{aligned}
				giveBirth(Lily, Mary)\\
				Female(Mary)\\
				\exists giveBirth.Person \sqsubseteq Female\\
				Female \sqsubseteq Person\\
				\end{aligned}
				\right\}.
				$$
			 
			 
			 对于所有的$\mathcal{S}' \subseteq \mathcal{S}$都有$\mathcal{S}' \not \models Female(Lily)$，且$\mathcal{S} \models Female(Lily)$，所以$\mathcal{S}$是$Female(Lily)$在本体$\mathcal{O}$中的决断集。
    \end{example}
	
	寻找决断集模板的方法是本体推理任务中重要的一环，寻找决断集的方法分为黑盒法和白盒法。
	
		\begin{figure}[htb]
  \center
  \includegraphics[width=0.85\textwidth]{img/12.jpg}\\
  \caption{决断集示例}\label{fig:fulladder}
	\end{figure}
	黑盒法需要依赖于本体的推理机，并且在方法的实现中不涉及推理机的改动，推理机在这类算法中扮演黑盒的角色，黑盒由此得名。推理机在黑盒算法中往往被应用在验证输入本体与蕴含值是否有蕴含关系，对于这类问题推理机只需要返回“是”或“否”。为了找到蕴含值的决断集，算法可以在本体中找到合理的子集作为推理机的输入，然后根据推理机的输出调整下一次输入，直到找到决断集为止。黑盒法需要有推理机运行蕴含值检测的次数较多，虽然易于实现，但是整体算法耗时一般偏大，以下是Kalyanpur等人提出的黑盒算法示例：
	\begin{algorithm}
  \caption{寻找单个决断集算法}
  \KwIn{Ontology $\mathcal{O}$, entailment $\alpha$ }
  \KwOut{Ontology $\mathcal{O}'$}

	$\mathcal{O}'\gets \emptyset $\
   
   
	\While{$\mathcal{O}' \not \models \alpha$}
	{
		select a set of axioms $s \subseteq \mathcal{O}\slash \mathcal{O}'$ \;
		$\mathcal{O}' \gets \mathcal{O}' \cup s$ \; 
	}
	
	\For{each axiom $k' \in \mathcal{O}'$}
	{
		$\mathcal{O}' \gets \mathcal{O}' - \{k'\}$ \;
		
		\If{$\mathcal{O}' \not \models \alpha$}
		{
			$\mathcal{O}' \gets \mathcal{O}' \cup \{k'\}$ \;
		}
	}
\end{algorithm}
	
	
	在黑盒算法中，主要分两步：1)快速地找到一个决断集的超集$\mathcal{O}'$使得$\mathcal{O}' \models \alpha$ 2)移除$\mathcal{O}'$中的一条或多条公理，使用推理机测试移除后得到的本体$\mathcal{O}'$是否可以蕴含$\alpha$，重复步骤1和2，就可以得到在$\mathcal{O}$中$\alpha$的一个决断集。可以看出，可以优化的关键步骤就在于每次添加公理和减少公理的个数，因为在整个算法中，推理机的蕴含检测是最为耗时的，减少推理机的运行次数可以有效地优化算法的耗时。
	
	白盒法计算决断集并不完全依赖于推理机的计算，其中一种白盒的算法是由Zhou等人提出的基于解释依赖图的决断集算法，这种算法可以在OWL 2 EL中计算本体$\mathcal{O}$蕴含值的决断集。解释依赖图算法需要先使用补全规则对本体$\mathcal{O}$进行蕴含值的计算，然后根据依赖图对本体中的蕴含值进行模板回溯。
	
	
\section{本体溯因诊断的研究}

	逻辑研究的是基于规则的推理方式，目前的研究中把推理的方式分为三类，分别是演绎、归纳和溯因推理。演绎推理是最常用的推理方式，演绎推理根据已有的前提事实以及规则，得出结论。对于相同的输入，如果严格按照规则进行运算，演绎推理具有相同的输出，具有恒真性(truth-preserving)。归纳推理则是在已知事实的集合中寻找共同特性，推导出更多事实或同类事实的性质[论语用推理的逻辑属性]。它的推理格式形如以下形式：
        \begin{enumerate}
               \item [a.]所有已知的A为B。
               \item [b.]因此，A为B。
        \end{enumerate}

    溯因推理是推理方式中的第三种方式，溯因推理的方式与前两种推理方式有着本质的区别，溯因推理又称作反绎推理，是推理到最佳解释的过程。一般的，它是开始于事实的集合并推导出它们的最合适的解释的推理过程。术语溯因（abduction）意味生成假设来解释观察或结论。因为需要生成假设来解释观察或结论，因此溯因推理会在进行解释的过程中为前提事实增加新的知识使得前提事实与解释的并集可以通过演绎推理的方式演绎出观察值或结论。在描述逻辑本体中，溯因推理是一种重要的推理方式。在描述逻辑本体的构建过程中，本体由于构建不够完善，会经常性出现本体无法蕴含观察值的现象。因此，这个时候就需要进行利用溯因推理的方法，已构建的本体进行诊断，找出本体不完善的原因，并在找到的原因的基础上，提出解释对本体进行修复。这种找出原因并提出解释的推理方法就叫做溯因诊断。

\subsection{断言公理的溯因诊断}
	
	溯因诊断对修复本体有着重要的意义。我们对术语断言的溯因诊断问题进行了如下两个定义[ABox Abduction in the Description Logic $\mathcal{ALC}$]：
    \begin{definition}[术语断言溯因诊断问题)]
        令$\mathcal{L}_K$和$\mathcal{L}_Q$为两个DL本体，$\mathcal{K}=(\mathcal{T}, \mathcal{A})$是一个$\mathcal{L}_K$中的一个知识库，$\Phi$是一个在$\mathcal{L}_Q$中的术语断言集合。当且仅当 $\mathcal{K} \nvDash \Phi$ 且$\mathcal{K} \cup \Phi \nvDash \bot$的时候，二元组$\langle \mathcal{K}, \Phi \rangle$被称作术语断言的溯因诊断问题。
    \end{definition}

    \begin{definition}[术语断言溯因诊断解释)]
        令$\mathcal{L}_S$为一个DL本体，且$A$为一个多个在$\mathcal{L}_S$中的术语断言集合。对于一个溯因诊断问题$\langle \mathcal{K}, \Phi \rangle$，当且仅当$\mathcal{K} \cup A \vDash \Phi$我们把$A$称为可接受解释。更多的，我们把$A$称为：
        \begin{enumerate}
               \item $(\emph{一致})$ 当且仅当$\mathcal{K} \cup A \nvDash \bot$。
               \item $(\emph{非平凡})$ 当且仅当 $\mathcal{K} \nvDash \Phi$。
               \item $(\emph{最小})$ 当且仅当不存在对于$\langle \mathcal{K}, \Phi \rangle$问题的解释$B$，其中$B$是$A$的实例化子集。我们说$B$是$A$的实例化子集当且仅当存在一个重命名映射$\rho$:$N_I^*(B) \mapsto N_I^*(A)$，其中$N_I^*(B)$和$N_I^*(A)$是来自于$A$和$B$的个体名且不出现于$\mathcal{K}$，使得$A \vDash \rho B$。但是对于所有的$\varrho$:$N_I^*(A) \mapsto N_I^*(B)$满足$B \nvDash \varrho A$。满足以上条件的，我们称$A$是问题$\langle \mathcal{K}, \Phi \rangle$的最小解释。
        \end{enumerate}

    \end{definition}
    
	术语断言的溯因诊断需要计算出由一条或多条术语断言的集合，这些集合需要满足最小集的条件。当这些集合被加入到本体中的时候，它需要保持本体的一致性，并且可以使得更新后的本体能够蕴含($\models$)含观察值。$\mathcal{O} \models \alpha$表示对于所有满足本体$\mathcal{O}$的模，都可以使得$\alpha$成立。
	
	
	考虑以下例子：
	\begin{example}
		令$\mathcal{O} = \mathcal{T} \cup \mathcal{A}$是一个DL 本体，其中$TBox$ $\mathcal{T}$和$ABox$ $\mathcal{A}$包含以下公理：
			
	$$ \mathcal{T}=\left\{
		\begin{aligned}
		Enthusiastic \sqcup (\exists owns.Cat \sqcap WorkHard) \sqsubseteq NicePerson\\
		Person \sqcap \exists help.Person \sqsubseteq Enthusiastic\\
		WorkHard \sqsubseteq Person
		\end{aligned}
	\right\}.
	$$
	
	$$ \mathcal{A}=\left\{
		\begin{aligned}
		Cat(\mathsf{Lily})\\
		Person(\mathsf{Peter})\\
		WorkHard(\mathsf{James})
		\end{aligned}
	\right\}.
	$$
	
	
	其中我们有观察值$\alpha$:
	$$ \alpha=\left\{
		\begin{aligned}
		NicePerson(\mathsf{James})
		\end{aligned}
	\right\}.
	$$
	
	根据给出的背景知识可以知道，一个热心的或者是一个工作努力又养猫的人是一个好人，并且乐于助人的人是热心的。又由$ABox$ $\mathcal
	{A}$可以知道Lily是一只猫，Peter是一个人以及James工作努力。
	由$TBox$ $\mathcal{T}$和$ABox$ $\mathcal{A}$，我们知道$\mathcal{O} \not \models \alpha$。从语义上说，溯因诊断的任务是提出一个合理的假设去解释“James是好人”。从技术上说，就是要找到合理的断言公理集合$\mathcal{E}$使得$\mathcal{O} \cup \mathcal{E} \models NicePerson(\mathsf{James})$。例如，在本例中，我们根据观察值和背景知识可以提出三个合理的解释：1)根据背景知识“热心人是好人”，我们可以假设“James是热心的”，即$\mathcal{E}=\{Enthusiastic(James)\}$，2)根据一个工作努力又养猫的人是一个好人，而James工作努力且Lily是一只猫，我们可以假设“James拥有Lily”，即$\mathcal{E}=\{owns(James, Lily)\}$，3)根据背景知识利于助人的人是热心人，热心人是好人，我们可以假设“James帮助了Peter”，即$\mathcal{E}=\{help(James, Peter)\}$。这三种假设都可以使观察值得出合理解释，从而使得$\mathcal{O} \cup \mathcal{E} \models \alpha$。
	\end{example}


	